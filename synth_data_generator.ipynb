{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## this notebook was developed on Google Colab\n"
      ],
      "metadata": {
        "id": "3JTmQi0xvsHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### dependencies"
      ],
      "metadata": {
        "id": "NSzPn-VCzYEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain_openai langchain_community"
      ],
      "metadata": {
        "id": "poAPvw5YUaLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "110963b5-4d4f-403d-fc3e-8cd77eed0258"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.8/122.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU --disable-pip-version-check qdrant-client pymupdf tiktoken"
      ],
      "metadata": {
        "id": "_HF0WQtiVaVx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "681ac7e3-4a49-4ca8-b98e-04114d6607a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.3/229.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.2/309.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.4 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-api-core 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-aiplatform 1.52.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-bigquery-connection 1.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-datastore 2.15.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-firestore 2.11.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-functions 1.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-iam 2.15.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-language 2.13.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-resource-manager 1.12.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "google-cloud-translate 3.11.3 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "googleapis-common-protos 1.63.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "grpc-google-iam-v1 0.13.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.27.0 which is incompatible.\n",
            "proto-plus 1.23.0 requires protobuf<5.0.0dev,>=3.19.0, but you have protobuf 5.27.0 which is incompatible.\n",
            "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.27.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.27.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "vAUf5Be4VA53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### rag chain"
      ],
      "metadata": {
        "id": "V5wzdFF9DY_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "PROMPT = \"\"\"\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUERY:\n",
        "{question}\n",
        "\n",
        "You are a helpful assistant. Use the available context to answer the question. If you can't answer the question, say you don't know.\n",
        "\"\"\"\n",
        "\n",
        "chain_prompt = ChatPromptTemplate.from_template(PROMPT)"
      ],
      "metadata": {
        "id": "G9sFAl9JDPxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "openai_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "qNT2SnQODV6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "llm_chain = (\n",
        "    {\"question\": itemgetter(\"question\")}\n",
        "    | chain_prompt | openai_chat_model | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "oV4Nit_vDXOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke({\"question\" : \"What does the 'context' in 'long context' refer to?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "bMCa7uQPDeUJ",
        "outputId": "e6bfff20-e2ac-409e-90b9-ac41ef7c2948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In the context of 'long context' in the provided text, the term 'context' refers to a coherent text, such as a book or a long paper, that is being used as the basis for generating question-answer pairs and analyzing information.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paper_title = llm_chain.invoke({\"question\" : \"Retrieve the paper title as string without any adittional word.\"})\n",
        "paper_title"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "naxWppQnOpjv",
        "outputId": "7265ff44-eeb5-4de3-f924-0e853074a898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Extending Llama-3’s Context'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tools"
      ],
      "metadata": {
        "id": "sufiMocGrFbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated, List, Tuple, Union\n",
        "\n",
        "def write_csv_file(content: Annotated[str, \"The string final content to write to the file.\"], filename: Annotated[str, \"The filename to save the file as, must be the dataset name. do NOT include .csv\"]):\n",
        "  \"\"\"Writes the given content as a csv file to the local directory.\n",
        "\n",
        "  Args:\n",
        "    content: The string content to write to the file.\n",
        "    filename: The filename to save the file as, must be the dataset name. do NOT include .csv\n",
        "  \"\"\"\n",
        "  with open(f\"{filename}.csv\", \"w\") as f:\n",
        "    f.write(content)"
      ],
      "metadata": {
        "id": "huI6-KcmcQY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_paper_title():\n",
        "  \"\"\"Returns the paper title to use later\"\"\"\n",
        "  return rag_chain.invoke({\"question\" : \"Retrieve the paper title as string without any adittional word.\"})"
      ],
      "metadata": {
        "id": "D16RapGFrN2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_information(query: Annotated[str, \"query to ask the retrieve information tool\"], paper_title: Annotated[str, 'title of the paper']):\n",
        "    f\"\"\"Use Retrieval Augmented Generation to retrieve information about {paper_title} paper.\"\"\"\n",
        "    return rag_chain.invoke({\"question\" : query})"
      ],
      "metadata": {
        "id": "jsdb3UQ1MNFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain.agents import AgentExecutor, create_react_agent, load_tools, Tool\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.tools import StructuredTool\n",
        "\n",
        "retrieve_paper_title = StructuredTool.from_function(\n",
        "                func=retrieve_paper_title,\n",
        "                name='RetrievePaperTitle',\n",
        "                description=\"retrieve the paper's title\")\n",
        "\n",
        "retrieve_information = StructuredTool.from_function(\n",
        "                func= retrieve_information,\n",
        "                name= 'RetrieveInformation',\n",
        "                description= 'search for information about the given paper using Retrieval Augmented Generation.',)\n",
        "\n",
        "write_content = StructuredTool.from_function(\n",
        "                func=write_markdown_file,\n",
        "                name=\"WriteContent\",\n",
        "                description=\"write the final content as a markdown file to the local directory.\",)\n",
        "\n",
        "\n",
        "tools = [retrieve_paper_title, retrieve_information, write_content]"
      ],
      "metadata": {
        "id": "1GMuuRH1pQlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0.0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Make sure to use the retrieve_information tool for information.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "-iIp3L60Fg5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "agent = create_tool_calling_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)#, handle_parsing_errors=True)"
      ],
      "metadata": {
        "id": "5NgbGy1Eo5RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = agent_executor.invoke(\n",
        "    {\n",
        "      \"input\": \"\"\"Identify and answer the following questions based on the paper content and reasearch:\n",
        "        -What is the research problem the paper attempts to address? What is the motivation?\n",
        "        -What are the claimed contributions and novelties of the paper?\n",
        "        -How do the authors substantiate their claims (methodology, experiments, proofs etc.)?\n",
        "        -What are the main conclusions and lessons learned?\n",
        "        -Is the research problem significant and not artificial?\n",
        "        -Are the claimed contributions really significant and novel compared to existing work?\n",
        "        -Are the claims and arguments valid, or are there flaws in the methodology, proofs, experiments etc.?\n",
        "        -What is the core of the research problem? Are there alternative approaches?\n",
        "        -Are there different ways to substantiate or argue against the claims?\n",
        "        -Can the results be strengthened or applied to other contexts?\n",
        "        -What are the open problems raised that could lead to further research?\n",
        "\n",
        "        Then, based on the questions and answers, use and complete the following template to write the final content to the disk using the paper's title as filename:\n",
        "\n",
        "        ## ReplaceThisWithPaperTitle\n",
        "\n",
        "        ### Comprehension:\n",
        "        \\n- **Research Problem and Motivation:**\n",
        "        \\n- **Claimed Contributions and Novelties:**\n",
        "        \\n- **Substantiation of Claims:**\n",
        "        \\n- **Main Conclusions and Lessons Learned:**\n",
        "\n",
        "        ### Evaluation:\n",
        "        \\n- **Significance of Research Problem:**\n",
        "        \\n- **Significance and Novelty of Contributions:**\n",
        "        \\n- **Validity of Claims and Arguments:**\n",
        "\n",
        "        ### Synthesis:\n",
        "        \\n- **Core Research Problem:**\n",
        "        \\n- **Alternative Approaches and Substantiation:**\n",
        "        \\n- **Strengthening and Application of Results:**\n",
        "        \\n- **Open Problems for Further Research:**\n",
        "        \"\"\",\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "MAV2zdI8ul1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d50857c4-573a-49cf-bc60-4159a3d435e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrievePaperTitle` with `{}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mExtending Llama-3’s Context\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'research problem and motivation of the paper', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mThe research problem addressed in the paper is the extension of long-context capabilities for large language models (LLMs) in an efficient manner. The motivation behind the paper is to explore the potential of LLMs to extend their original context length beyond current limitations, with the goal of enhancing performance across various evaluation tasks while preserving capabilities over short contexts.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'claimed contributions and novelties of the paper', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mThe paper \"Extending Llama-3’s Context Ten-Fold Overnight\" claims to have extended the context length of Llama-3-8B-Instruct from 8K to 80K using QLoRA fine-tuning. The authors state that the resulting model shows superior performance across various evaluation tasks such as NIHS, topic retrieval, and long-context language understanding. They also mention that the model preserves its original capability over short contexts while dramatically extending the context length. The authors attribute this extension to the generation of synthetic training samples by GPT-4. They further suggest that the context length could be extended even further with more computational resources and intend to publicly release all related resources.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'methodology, experiments, and proofs used in the paper', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mThe available context does not provide specific details about the methodology, experiments, and proofs used in the paper. Therefore, I don't have the information to answer your question.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'main conclusions and lessons learned from the paper', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mThe main conclusions from the paper include:\n",
            "- Long-context models may underperform compared to the original Llama-3-8B-Instruct model when context is extended, compromising short-context capability.\n",
            "- The model developed in the paper still outperforms other open-source models at the same scale.\n",
            "- The performance of the model was evaluated on various benchmarks, showing superior results in most cases.\n",
            "- There is a metric-oriented issue in summarization tasks, where different paraphrases may affect the evaluation results.\n",
            "\n",
            "Overall, the paper highlights the importance of balancing context extension with model performance and the need for careful consideration of evaluation metrics in long-context tasks.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'significance of the research problem and whether it is artificial', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mBased on the provided context, the research problem discussed is focused on extending the context length of large language models like Llama-3-8B-Instruct3 from 8K to 80K using GPT-4. The significance of this research problem lies in enhancing the long-context capabilities of language models, which can improve their performance in tasks such as question-answering and text summarization. The research is artificial in nature as it involves the use of artificial intelligence models like GPT-4 to generate synthetic training data and extend the context length of the language model.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'comparison of claimed contributions with existing work', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mBased on the provided context, there is a comparison of the model Llama-3-8B-Instruct with other baselines and models such as GPT-4 in terms of performance on tasks involving long contexts. The results show that Llama-3-8B-Instruct outperforms all baselines on most tasks except for the code completion task. Additionally, Llama-3-8B-Instruct performs competitively against GPT-4 on tasks like answering questions and summarization, even outperforming GPT-4 in some cases. This comparison highlights the claimed contributions of Llama-3-8B-Instruct in handling long-context tasks effectively.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `RetrieveInformation` with `{'query': 'validity of claims and arguments in the paper', 'paper_title': 'Extending Llama-3’s Context'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[33;1m\u001b[1;3mBased on the provided context, the paper discusses the evaluation results of different models on long-context tasks such as Topic Retrieval, LongBench, and MMLU benchmarks. The paper compares the performance of their model with baselines and mentions that their model outperforms other open-source models at the same scale. It also highlights the potential trade-off between long-context capabilities and short-context performance. The paper provides references to previous research and open-source models for comparison.\n",
            "\n",
            "Overall, the paper presents a detailed analysis of the model's performance and compares it with other models, providing evidence to support their claims. However, without a detailed analysis of the specific claims and arguments made in the paper, it is difficult to determine the validity of each individual claim.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `WriteContent` with `{'content': \"## Extending Llama-3’s Context\\n\\n### Comprehension:\\n\\n- **Research Problem and Motivation:**\\n  The research problem addressed in the paper is the extension of long-context capabilities for large language models (LLMs) in an efficient manner. The motivation behind the paper is to explore the potential of LLMs to extend their original context length beyond current limitations, with the goal of enhancing performance across various evaluation tasks while preserving capabilities over short contexts.\\n\\n- **Claimed Contributions and Novelties:**\\n  The paper claims to have extended the context length of Llama-3-8B-Instruct from 8K to 80K using QLoRA fine-tuning. The authors state that the resulting model shows superior performance across various evaluation tasks such as NIHS, topic retrieval, and long-context language understanding. They also mention that the model preserves its original capability over short contexts while dramatically extending the context length. The authors attribute this extension to the generation of synthetic training samples by GPT-4.\\n\\n- **Substantiation of Claims:**\\n  The paper provides evidence of the model's performance compared to baselines and other models on tasks involving long contexts. It discusses the trade-off between long-context capabilities and short-context performance, highlighting the model's competitive performance.\\n\\n- **Main Conclusions and Lessons Learned:**\\n  The main conclusions include the importance of balancing context extension with model performance and the need for careful consideration of evaluation metrics in long-context tasks.\\n\\n### Evaluation:\\n\\n- **Significance of Research Problem:**\\n  The research problem of extending long-context capabilities for LLMs is significant as it can enhance performance in various tasks.\\n\\n- **Significance and Novelty of Contributions:**\\n  The claimed contributions of extending the context length of Llama-3-8B-Instruct and its performance on long-context tasks are significant and novel compared to existing work.\\n\\n- **Validity of Claims and Arguments:**\\n  The paper provides evidence to support its claims, but a detailed analysis of the validity of each claim is required.\\n\\n### Synthesis:\\n\\n- **Core Research Problem:**\\n  The core research problem is extending the context length of LLMs efficiently while maintaining performance.\\n\\n- **Alternative Approaches and Substantiation:**\\n  The paper discusses the use of GPT-4 for generating synthetic training samples to extend the context length.\\n\\n- **Strengthening and Application of Results:**\\n  The results could be strengthened by further analysis and comparison with other models.\\n\\n- **Open Problems for Further Research:**\\n  The paper raises questions about the trade-off between long-context capabilities and short-context performance, suggesting avenues for further research.\", 'filename': 'Extending Llama-3’s Context.md'}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[38;5;200m\u001b[1;3mNone\u001b[0m\u001b[32;1m\u001b[1;3mI have created the content based on the questions and answers related to the paper \"Extending Llama-3’s Context.\" The content has been saved as a markdown file with the title \"Extending Llama-3’s Context.md\" in the local directory.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output['output'])"
      ],
      "metadata": {
        "id": "oCd6WFnMbZfv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48994871-4eb7-47a7-dd8b-2c371290abee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have created the content based on the questions and answers related to the paper \"Extending Llama-3’s Context.\" The content has been saved as a markdown file with the title \"Extending Llama-3’s Context.md\" in the local directory.\n"
          ]
        }
      ]
    }
  ]
}